{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\k12s35h813g\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import math\n",
    "\n",
    "__author__ = \"Bo-Syun Cheng\"\n",
    "__email__ = \"k12s35h813g@gmail.com\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data_preprocessor():\n",
    "    def __init__(self,data,filter_user=1,filter_item=5):\n",
    "        self.data = data\n",
    "        self.filter_user = filter_user\n",
    "        self.filter_item = filter_item\n",
    "\n",
    "    def preprocess(self):\n",
    "        self.filter_()\n",
    "        return self.train_test_split()\n",
    "    def filter_(self):\n",
    "        \"\"\"\n",
    "        過濾掉session長度過短的user和評分數過少的item\n",
    "\n",
    "        :param filter_user: 少於這個session長度的user要被過濾掉 default=1\n",
    "        :param filter_item: 少於這個評分數的item要被過濾掉 default=5\n",
    "        :return: dataframe\n",
    "        \"\"\"\n",
    "        session_lengths = self.data.groupby('userId').size()\n",
    "        self.data = self.data[np.in1d(self.data['userId'], session_lengths[session_lengths>1].index)] #將長度不足2的session過濾掉\n",
    "        print(\"剩餘data : %d\"%(len(self.data)))\n",
    "        item_supports = self.data.groupby('movieId').size() #統計每個item被幾個使用者評過分\n",
    "        self.data = self.data[np.in1d(self.data['movieId'], item_supports[item_supports>5].index)] #將被評分次數低於5的item過濾掉\n",
    "        print(\"剩餘data : %d\"%(len(self.data)))\n",
    "        \"\"\"再把只有一個click的user過濾掉 因為過濾掉商品可能會導致新的單一click的user出現\"\"\"\n",
    "        session_lengths = self.data.groupby('userId').size()\n",
    "        self.data = self.data[np.in1d(self.data['userId'], session_lengths[session_lengths>1].index)]\n",
    "        print(\"剩餘data : %d\"%(len(self.data)))\n",
    "    def train_test_split(self,time_range=86400):\n",
    "        \"\"\"\n",
    "        切割訓練和測試資料集\n",
    "\n",
    "        :param time_range:session若在這個區間內，將被分為test_data default=86400(1day)\n",
    "        :retrun: a tuple of two dataframe\n",
    "        \"\"\"\n",
    "        tmax = self.data['timestamp'].max()\n",
    "        session_tmax = self.data.groupby('userId')['timestamp'].max()\n",
    "        train = self.data[np.in1d(self.data['userId'] , session_tmax[session_tmax<=tmax -86400].index)]\n",
    "        test = self.data[np.in1d(self.data['userId'] , session_tmax[session_tmax>tmax -86400].index)]\n",
    "        print(\"訓練資料集統計:  session個數:%d , item個數:%d , event數:%d\"%(train['userId'].nunique(),train['movieId'].nunique(),len(train)))\n",
    "        \"\"\"\n",
    "        基於協同式過濾的特性，若test data中含有train data沒出現過的item，將該item過濾掉\n",
    "        \"\"\"\n",
    "        test = test[np.in1d(test['movieId'], train['movieId'])]\n",
    "        tslength = test.groupby('userId').size()\n",
    "        test = test[np.in1d(test['userId'], tslength[tslength>=2].index)]\n",
    "        print(\"測試資料集統計:  session個數:%d , item個數:%d , event數:%d\"%(test['userId'].nunique(),test['movieId'].nunique(),len(test)))\n",
    "\n",
    "        return train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BPR():\n",
    "    '''\n",
    "    parameter\n",
    "    train_sample_size : 訓練時，每個正樣本，我sample多少負樣本\n",
    "    test_sample_size : 測試時，每個正樣本，我sample多少負樣本\n",
    "    num_k : item embedding的維度大小\n",
    "    evaluation_at : recall@多少，及正樣本要排前幾名，我們才視為推薦正確\n",
    "    '''\n",
    "    def __init__(self,data,n_epochs=10,batch_size=32,train_sample_size=10,test_sample_size=50,num_k=100,evaluation_at=10):\n",
    "        self.n_epochs = n_epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.train_sample_size = train_sample_size\n",
    "        self.test_sample_size = test_sample_size\n",
    "        self.num_k = num_k\n",
    "        self.evaluation_at = evaluation_at\n",
    "\n",
    "        self.data = data\n",
    "        self.num_user = len(self.data['userId'].unique())\n",
    "        self.num_item = len(self.data['movieId'].unique())\n",
    "        self.num_event = len(self.data)\n",
    "\n",
    "        self.all_item = set(self.data['movieId'].unique())\n",
    "        self.experiment = []\n",
    "\n",
    "        #Because the id is not always continuous , we build a map to normalize id . For example:[1,3,5,156]->[0,1,2,3]\n",
    "        user_id = self.data['userId'].unique()\n",
    "        self.user_id_map = {user_id[i] : i for i in range(self.num_user)}\n",
    "        item_id = self.data['movieId'].unique()\n",
    "        self.item_id_map = {item_id[i] : i for i in range(self.num_item)}\n",
    "        training_data = self.data.loc[:,['userId','movieId']].values\n",
    "        self.training_data = [[self.user_id_map[training_data[i][0]],self.item_id_map[training_data[i][1]]] for i in range(self.num_event)]\n",
    "\n",
    "\n",
    "\n",
    "        #data preprocess\n",
    "        self.split_data() #split data into training_data and testing\n",
    "        self.sample_dict = self.negative_sample() #for each trainging data (user,item+) , we sample 10 negative item for bpr training\n",
    "\n",
    "        self.build_model() #build TF graph\n",
    "        self.sess = tf.Session() #create session\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "\n",
    "\n",
    "    def split_data(self):\n",
    "        user_session = self.data.groupby('userId')['movieId'].apply(set).reset_index().loc[:,['movieId']].values.reshape(-1)\n",
    "        self.testing_data =[]\n",
    "        for index,session in enumerate(user_session):\n",
    "            random_pick = self.item_id_map[random.sample(session,1)[0]]\n",
    "            self.training_data.remove([index,random_pick])\n",
    "            self.testing_data.append([index,random_pick])\n",
    "\n",
    "\n",
    "    def negative_sample(self):\n",
    "        user_session = self.data.groupby('userId')['movieId'].apply(set).reset_index().loc[:,['movieId']].values.reshape(-1)\n",
    "        sample_dict = {}\n",
    "\n",
    "        for td in self.training_data:\n",
    "            sample_dict[tuple(td)] = [self.item_id_map[s] for s in random.sample(self.all_item.difference(user_session[td[0]]) , self.train_sample_size)]\n",
    "\n",
    "        return sample_dict\n",
    "\n",
    "    def build_model(self):\n",
    "        self.X_user = tf.placeholder(tf.int32,shape=(None , 1))\n",
    "        self.X_pos_item = tf.placeholder(tf.int32,shape=(None , 1))\n",
    "        self.X_neg_item = tf.placeholder(tf.int32,shape=(None , 1))\n",
    "        self.X_predict = tf.placeholder(tf.int32,shape=(1))\n",
    "\n",
    "        user_embedding = tf.Variable(tf.truncated_normal(shape=[self.num_user,self.num_k],mean=0.0,stddev=0.5))\n",
    "        item_embedding = tf.Variable(tf.truncated_normal(shape=[self.num_item,self.num_k],mean=0.0,stddev=0.5))\n",
    "\n",
    "        embed_user = tf.nn.embedding_lookup(user_embedding , self.X_user)\n",
    "        embed_pos_item = tf.nn.embedding_lookup(item_embedding , self.X_pos_item)\n",
    "        embed_neg_item = tf.nn.embedding_lookup(item_embedding , self.X_neg_item)\n",
    "\n",
    "        pos_score = tf.matmul(embed_user , embed_pos_item , transpose_b=True)\n",
    "        neg_score = tf.matmul(embed_user , embed_neg_item , transpose_b=True)\n",
    "\n",
    "        self.loss = tf.reduce_mean(-tf.log(tf.nn.sigmoid(pos_score-neg_score)))\n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate=0.001).minimize(self.loss)\n",
    "\n",
    "        predict_user_embed = tf.nn.embedding_lookup(user_embedding , self.X_predict)\n",
    "        self.predict = tf.matmul(predict_user_embed , item_embedding , transpose_b=True)\n",
    "\n",
    "    def fit(self):\n",
    "        self.experiment = []\n",
    "        for epoch in range(self.n_epochs):\n",
    "            np.random.shuffle(self.training_data)\n",
    "            total_loss = 0\n",
    "            for i in range(0 , len(self.training_data) , self.batch_size):\n",
    "                training_batch = self.training_data[i:i+self.batch_size]\n",
    "                user_id = []\n",
    "                pos_item_id = []\n",
    "                neg_item_id = []\n",
    "                for single_training in training_batch:\n",
    "                    for neg_sample in list(self.sample_dict[tuple(single_training)]):\n",
    "                        user_id.append(single_training[0])\n",
    "                        pos_item_id.append(single_training[1])\n",
    "                        neg_item_id.append(neg_sample)\n",
    "\n",
    "                user_id = np.array(user_id).reshape(-1,1)\n",
    "                pos_item_id = np.array(pos_item_id).reshape(-1,1)\n",
    "                neg_item_id = np.array(neg_item_id).reshape(-1,1)\n",
    "\n",
    "                _ , loss = self.sess.run([self.optimizer , self.loss] ,\n",
    "                           feed_dict = {self.X_user : user_id , self.X_pos_item : pos_item_id , self.X_neg_item : neg_item_id}\n",
    "                           )\n",
    "                total_loss += loss\n",
    "\n",
    "            num_true = 0\n",
    "            for test in self.testing_data:\n",
    "                result = self.sess.run(self.predict , feed_dict = {self.X_predict : [test[0]]})\n",
    "                result = result.reshape(-1)\n",
    "                if (result[[self.item_id_map[s] for s in random.sample(self.all_item , self.test_sample_size)]] > result[test[1]]).sum()+1 <= self.evaluation_at:\n",
    "                    num_true += 1\n",
    "\n",
    "            print(\"epoch:%d , loss:%.2f , recall:%.2f\"%(epoch , total_loss , num_true/len(self.testing_data)))\n",
    "            self.experiment.append([epoch , total_loss , num_true/len(self.testing_data)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "剩餘data : 100004\n",
      "剩餘data : 88087\n",
      "剩餘data : 88087\n",
      "訓練資料集統計:  session個數:669 , item個數:3099 , event數:86870\n",
      "測試資料集統計:  session個數:2 , item個數:1154 , event數:1217\n",
      "epoch:0 , loss:3196.12 , recall:0.23\n",
      "epoch:1 , loss:2039.38 , recall:0.27\n",
      "epoch:2 , loss:1141.34 , recall:0.41\n",
      "epoch:3 , loss:668.30 , recall:0.54\n",
      "epoch:4 , loss:427.90 , recall:0.58\n",
      "epoch:5 , loss:282.31 , recall:0.63\n",
      "epoch:6 , loss:187.26 , recall:0.63\n",
      "epoch:7 , loss:123.10 , recall:0.64\n",
      "epoch:8 , loss:79.32 , recall:0.67\n",
      "epoch:9 , loss:49.86 , recall:0.67\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    data = pd.read_csv('ratings_small.csv')\n",
    "    dp = Data_preprocessor(data)\n",
    "    processed_data = dp.preprocess()\n",
    "    \n",
    "    bpr = BPR(processed_data)\n",
    "    bpr.fit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
